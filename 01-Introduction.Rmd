---
output:
  html_document: default
  pdf_document: default
---
# Introduction

## What is Multiple Testing

In various scientific applications we wish to test several hypotheses of interest
simultaneously or sequentially^[We'll come back to sequential testing later].
The popular example is Genome Wide Association Studies which focus on (amongst 
other things) the association of several genes with a known disease type.

This leads to the reproducibility crisis in science. Too many 
false discoveries published in papers due to not adjusting Type I 
error control for multiple tests.

```{remark}
Is this is the only reason for the reproducibility crisis? Or is there
more to it i.e. not releasing data and code publicly for 
research publications? Check with Ron.
```

## $\text{FWER}$ Control methods

### Bonferroni Correction - $\text{FWER}$ under dependence

Suppose we have that the tests i.e. the p-values are dependent. We can then
use the Bonferroni correction for each p-value

```{theorem, name="Bonferroni Correction"}
For $d$ tests, reject any test if the p-value is smaller than $\frac{\alpha}{d}$.
```

```{proof}
$$\begin{align*}
\text{FWER} &= \mathbb{P}\left(\cup_{i = 1}^{d} \text{Falsely reject } H_{0, i} \right) \\
&\le \sum_{i = 1}^{d} \mathbb{P}\left(\text{Falsely reject } H_{0, i}\right) \\
&\le \sum_{i = 1}^{d} \frac{\alpha}{d} \\
&= d \left(\frac{\alpha}{d} \right) \\
&= \alpha
\end{align*}$$
as required.
```


```{remark}
This works under any form of dependence of the tests i.e. p-values since
the proof uses Union Bound. This may be quite crude a threshold as a result.
```

### Sidak Correction - $\text{FWER}$ under independence

Suppose we have that the tests i.e. the p-values are now __independent__. We we can then use the Sidak correction to set the p-value threshold to control
the $\text{FWER}$ at level $\alpha$.

```{theorem, name="Sidak Correction"}
For $d$ tests, reject any test if the p-value is smaller than $\frac{\alpha}{d}$.
```

```{proof}
$$\begin{align*}
\text{FWER} &= \mathbb{P}\left(\cup_{i = 1}^{d} \text{Falsely reject } H_{0, i} \right) \\
&= 1 - \mathbb{P}\left(\cap_{i = 1}^{d} \{\text{Falsely reject } H_{0, i}\}^{c} \right) \\
&= 1 - \prod_{i=1}^{d}{\mathbb{P}\left(\{\text{Falsely reject } H_{0, i}\}^{c} \right)} \\
&= 1 - \prod_{i=1}^{d}(1 - \mathbb{P}\left(\text{Falsely reject } H_{0, i} \right)) \\
&= 1 - \prod_{i=1}^{d}(1 - (1 - (1 - \alpha)^{\frac{1}{d}})) \\
&= 1 - \prod_{i=1}^{d}(1 - \alpha)^{\frac{1}{d}} \\
&= 1 - (1 - \alpha) \\
&= \alpha
\end{align*}$$
as required.
```

```{remark}
This procedure specifically requires independence of the tests i.e. p-values since. The gain from the indepedence strucuture here results in a more powerful test than Bonferroni (prove this!).
```

### Holm's Procedure - Improving on Bonferroni

We can improve on the Bonferroni test in various ways. If we assume a bit
more knowledge about our experiment and note that out of $d$ tests that
$d_0$ are true nulls. Let us denote the indices of all tests as $I$ and the
indices of the true nulls as $I_0$. Then we have by definition $I_0 \subseteq I$. In this case we can just use the p-value cut-off value of $\frac{\alpha}{d_0}$ and control the $\text{FWER}$ at level $\alpha$.

Equipped with this intuition, Holm's procedure is as follows:

```{theorem, name="Holm's Procedure"}
Suppose we have ordered p-values for the $d$ tests as $p_{(i)}$ for all $i \in d$. Then Holm's controls FWER by rejecting all $H_{(i)}$ for $i < i^{*}$ where:
$$i^{*} = \min \left\{i : p_{(i)} > \frac{\alpha}{d - i + 1 } \right\}$$
```

```{proof}
Let $I_0$ denote the true nulls from the set of all hypothesis $I$. We then
have that if:
    $$\min_{i \in I_0}(p_i) > \frac{\alpha}{d_0}$$
then we would reject none of the true nulls. This is because the first time
we would encounter a true null, we would compare it to a threshold that is
at most $\frac{\alpha}{d_0}$ and if we fail to reject it, then we would not reject any of the true nulls.

So the $\text{FWER}$ is:
$$\begin{align*}
\text{FWER} &= \mathbb{P}\left(\min_{i \in I_0}(p_i) \le \frac{\alpha}{d_0} \right) \\
&= \mathbb{P}\left(\bigcup_{i \in I_0} \mathbb{P}\left\{p_i \le \frac{\alpha}{d_0} \right\} \right) \\
&\le \sum_{i \in I_0} \mathbb{P}\left(p_i \le \frac{\alpha}{d_0} \right) \\
&= d_0 \left(\frac{\alpha}{d_0}\right) \\
&= \alpha
\end{align*}$$
as required.
```

```{remark}
Holm's procedure controls $\text{FWER}$ at level $\alpha$ and does not require the independence of p-values (similar to Bonferroni). However
it strictly dominates Bonferroni in the sense of higher power (CHECK THIS, PROVE IT)!
```



## Correcting for Multiple Testing

There are several ways to correct for false discovery in the case of
multiple testing procedures. Once again in the spirit of the Neyman Pearson
framework we want to ensure that we first control for the Type I error i.e.
the probability of False Discovery. Specifically in single hypothesis testing
we want to ensure that 

One way is to control the Familywise Error Rate i.e. $\text{FWER}$. This is the probability of making atleast one false
discovery (i.e. rejecting the null when the null is true) amongst all tested
hypotheses. This is a quite conservative approach to FD control.




## Relationships between $\text{FDR}$ and $\text{FWER}$

We note the following 2 key theorems which link $\text{FDR}$ and $\text{FWER}$.

```{theorem, name="FDR = FWER under Global Null"}
Under the Global Null we have that FDR control is equivalent to FWER control
```

```{proof}
Under the global null ^[i.e. all null hypotheses are assumed to be true] we have
that all rejections are false rejections i.e. $V = R$. We first consider two 
cases namely that there are no false rejections $V = 0$ and 
secondly if there are any false rejections at all i.e. $V \ge 1$.

When $V = 0$ then we have that $\text{FDP} = 0$ else if $V \ge 1$ we have 
that $\text{FDP} = 1$.

So we have that:
    
$$\begin{align}\text{FDR} = \mathbb{E}\left(\text{FDP} \right) 
                          &= (0) \times \mathbb{P}(V = 0) + 
                             (1) \times \mathbb{P}(V \ge 1) \\
&= \mathbb{P}(V \ge 1) \\
&= \text{FWER}
\end{align}$$
as required.    
```

This now brings us to the following important theorem^[In other words 
$\text{FWER}$ control is a more conservative method of controlling False 
Discovery in multiple testing than $\text{FDR}$ control.]:

```{theorem, name="FWER control always implies FDR control"}
We always have that $\text{FWER}$ control implies $\text{FDR}$
control.
```

```{proof}
We begin with the simple observation that $FDP = \frac{V}{R} \mathbb{I}(R > 0)$.
So we always have $FDP \le 1$ and more specifically we have that $FDP \le \mathbb{I}(V > 0)$.
Since if $V = 0 \implies FDP = 0 \le 1$ and $V \ge 1 \implies FDP = \frac{V}{R} \le 1$.

Taking expectations we have that:
    
$$\begin{align*}\text{FDR} = \mathbb{E}\left(\text{FDP} \right) &\le 
                             \mathbb{E}\left(\mathbb{I}(V > 0) \right) \\
                            &= \mathbb{P}(V \ge 1) \\                                                            
                            &= \text{FWER}
\end{align*}$$
as required.    
```